{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Workbook 00\n",
    "\n",
    "**For these questions, assume that an $x$ input has 1024 dimensions, that the first hidden layer should have $512$ units, a second layer has $256$ units, and that there are $10$ classes to choose from at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell to run for Latex commands**\n",
    "\n",
    "\\\\[\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\grad}[1]{\\nabla #1}\n",
    "\\newcommand{\\softmax}[0]{\\text{SOFTMAX}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about shapes of $X$, $W^{(i)}$, $b^{(i)}$ and $Z^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What do the rows of $X$ represent? What do the columns of $X$ represent? What is the shape of $X$?**\n",
    "\n",
    "Rows represent each image in the batch.\n",
    "\n",
    "Columns represent each pixel in the image; a column of values is the same pixel's value in each of the images.\n",
    "\n",
    "The shape of $X$ is $(bs, 1024)$. $bs$ is the \"batch size\", or the number of $x$ examples we want to simultaneously train on in our \"batch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. You have a first matrix of weights $W^{(1)}$ and a vector of biases $b^{(1)}$. What are the shapes of $W^{(1)}$ and $b^{(1)}$? Why have I written these superscripts?**\n",
    "\n",
    "The shape of $W^{(1)}$ is: $(1024, 512)$. The shape of $b^{(1)}$ is $(1, 512)$.\n",
    "\n",
    "This is the first set of weights and biases used to calculate the pre-activations for the first hidden layer. There will be more weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. For a single $x$ input of shape $(1, 1024)$, what is the formula to calculate the hidden pre-activation values $z^{(1)}$? What is the dimensionality of $z^{(1)}$?**\n",
    "\n",
    "Formula is:\n",
    "\n",
    "\\\\[\n",
    "z^{(1)} = x W^{(1)} + b^{(1)}\n",
    "\\\\]\n",
    "\n",
    "The dimensionality of $z^{(1)}$ is $(1, 512)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. For a batch input matrix $X$ of shape $(bs, 1024)$, what is the formula to calculate the hidden pre-activation values $Z^{(1)}$? What is the dimensionality of $Z^{(1)}$?**\n",
    "\n",
    "**Hint**: in numpy, if $A$ has dimension $(10, 24)$, then you can add $b$ a vector of dimension $(1, 24)$, and $A + b$ is the $(10, 24)$ matrix where each row of $A$ has the vector $b$ added to it. This is called a *broadcasting* addition. You may write the formula with $+$ interpreted to allow broadcasting addition.\n",
    "\n",
    "\\\\[\n",
    "Z^{(1)} = X W^{(1)} + b^{(1)}\n",
    "\\\\]\n",
    "\n",
    "The dimensionality of $Z^{(1)}$ is $(bs, 512)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the $\\sigma$ function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do I convert a $3:1$ odds of winning to a $75$% probability of winning? If the odds of an event are $odds:1$, how do I convert that to a percentage?**\n",
    "\n",
    "If the odds are $odds:1$, then the formula for $odds$ to $p$ is $p = \\frac{odds}{1 + odds}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The $\\sigma$ function turns a number from $(-\\infty, \\infty)$ into a number from $(0, 1)$ that can be used as a probability. What is the formula for $\\sigma(z)$? Give me the version with $e^z$ in the numerator.**\n",
    "\n",
    "$\\sigma(z) = \\frac{e^z}{1 + e^z}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Let's say $f(odds) = \\frac{odds}{1 + odds}$. $f$ converts and odds to a probability. Can you write sigmoid in terms of $f$?**\n",
    "\n",
    "$\\sigma(z) = f(e^z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. If we usually interpret the input of $f$ as an odds, then if we try to interpret $e^z$ as an odds, what does that imply we would interpret $z$ as?**\n",
    "\n",
    "We interpret $z$ as the log of an odds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a. What $z$ value has $\\sigma(z) = 0.50$ (50% probability) equivalent to an odds of $1.0$?**\n",
    "\n",
    "$z = 0.0$.\n",
    "\n",
    "**5b. When is the probability $<0.5$, when is the probability $>0.5$?**\n",
    "\n",
    "When $z$ is negative the probability will be less than half, when $z$ is positive probability will be greater than half.\n",
    "\n",
    "Note: $\\sigma(z)$ isn't necessarily a probability. It can be the \"percent activated.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6a. What is the problem numerically with the $\\sigma(z) = \\frac{e^z}{1+e^z}$ formula? What happens when $z$ is really large? What will we calculate on our CPU? What do we *want* to calculate theoretically?**\n",
    "\n",
    "When $z$ is really large then the floating point representation of $e^z$ can overflow and be $\\infty$.\n",
    "\n",
    "That's a problem because both the numerator and denominator will be $\\infty$ which means their ratio is not a number. We want it to be: $1.0$.\n",
    "\n",
    "**6b. Is there a problem for very negative $z$s with this formula?**\n",
    "\n",
    "No, because $e^z$ will round to $0.0$ and that's not a problem because this is zero divided by one which is zero which is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. How do we fix this problem? What's the better formula for a computer?**\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{e^{-z} + 1}$. It works for very negative and very positive $z$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and Pre-Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do I calculate the activation values $H^{(1)}$ from $Z^{(1)}$?**\n",
    "\n",
    "\\\\[\n",
    "H^{(1)} = \\sigma(Z^{(1)})\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is another name for a function like $\\sigma$ when used to calculate hidden activations? What are other examples?**\n",
    "\n",
    "Activation function. ReLU, softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What do I call the linear transformation of the $X$ values before being input into the activation function? What symbols do I use?**\n",
    "\n",
    "We wrote it as $z^{(1)}$ and it is called pre-activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the purpose of an activation function? What is an interpretation of the activation function $\\sigma$?**\n",
    "\n",
    "Without activation functions, we just have a series of linear functions, which is equivalent to just a single linear function.\n",
    "\n",
    "We want the neurons to be able to represent binary feature detectors: the feature is present or not present. Thus it makes sense to reduce to a range of $(0, 1)$ with zero meaning \"not detected\" and one meaning \"detected.\"\n",
    "\n",
    "Intermediate values mean \"sort-of detected.\"\n",
    "\n",
    "Because of the non-linearity, subsequent layers look for features in the first-layer features.\n",
    "\n",
    "**TODO**: Something something universal function approximators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating columns and rows of $W^{(i)}$ and $Z^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do we notate the $i$th row of $W^{(1)}$? The $j$th column?**\n",
    "\n",
    "$W^{(1)}_{i, :}$ and $W^{(1)}_{:, j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the formula for calculating a specific pre-activation $z^{(1)}_j$ for a single input $x$? Write as a vector operation with a dot-product. The write the formula replacing the dot-product with an explicit sum using $\\sum$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z^{(1)}_j &= x \\cdot W^{(1)}_{:, j} + b^{(1)}_j\n",
    "\\\\\n",
    "z^{(1)}_j &= \\left(\n",
    "    \\sum_{i = 0}^{1024} x_i W^{(1)}_{i, j}\n",
    "\\right) + b^{(1)}_j\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What does a column of $W^{(1)}_{:, j}$ represent? What does a row of $W^{(1)}_{i, :}$ represent?**\n",
    "\n",
    "The column consists of weights for each input dimension $x_i$ used to calculate the preactivation $z^{(1)}_j$. They are the weights for the $j$th hidden unit.\n",
    "\n",
    "The row consists of weights for a single input dimension $x_i$ used to compute the contribution of $x_i$ to each of the hidden pre-activations $z^{(1)}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Hidden Layer, Output Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the dimensions of $W^{(2)}$ and $b^{(2)}$?**\n",
    "\n",
    "$(512, 256)$ and $(1, 256)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the dimension of the first-layer activations $H^{(1)}$?**\n",
    "\n",
    "$(bs, 512)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the formulas for $Z^{(2)}$ and $H^{(2)}$?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(2)} &= H^{(1)} W^{(2)} + b^{(2)}\n",
    "\\\\\n",
    "H^{(2)} &= \\sigma(Z^{(2)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What does the row $W^{(2)}_{i, :}$ represent? What does the column $W^{(2)}_{:, j}$ represent?**\n",
    "\n",
    "The $j$th column of $W^{(2)}$ represents the weights for each output of the first hidden layer used to compute the $j$th pre-activation of the second hidden layer.\n",
    "\n",
    "The $i$th row of $W^{(2)}$ is all of the weights for the $i$th activation of the hidden layer $H^{(1)}$ used to compute the contribution of the $i$th hidden unit of the first layer all the hidden pre-activations $z^{(2)}_j$ for all $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the dimensions of $W^{(3)}, b^{(3)}$? What is the shape of $Z^{(3)}$?**\n",
    "\n",
    "$(256, 10)$ and $(1, 10)$. $(bs, 10)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the formula for calculating $Z^{(3)}$?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "Z^{(3)} &= H^{(2)} W^{(3)} + b^{(3)}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer Activations: $\\softmax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the formula for calculating $H^{(3)}$? Hint: you don't use $\\sigma$ this time!**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "H^{(3)} &= \\softmax(Z^{(3)})\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The $\\softmax$ function maps a 10-dimensional vector $z^{(3)}$ to a 10-dimensional vector $h^{(3)}$. How is $h^{(3)}_i$ calculated?**\n",
    "\n",
    "\\\\[\n",
    "h^{(3)}_i = \\frac{\n",
    "    \\exp \\left(z^{(3)}_i\\right)\n",
    "}{\n",
    "    \\sum_{j = 0}^9 \\exp \\left(z^{(3)}_j\\right)\n",
    "}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How is the $\\softmax$ function like the $\\sigma$ function?**\n",
    "\n",
    "**TODO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Outputs $y^*$, $y$, and $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In our problem, we want to classify an input $x$ as one of ten classes. Let's represent the correct answer in our training dataset as $y^*$. What is the shape of $y^*$? What is its range of values?**\n",
    "\n",
    "Shape of $y^*$ is `()` or just a scalar. The range is zero to nine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We will denote the one-hot encoding of $y^*$ as simply $y$. What is the shape and range of the values of $y$?**\n",
    "\n",
    "This is a ten-dimensional vector, where all the values are zero, except at one position. At the position $y^*$, the value is $1.0$.\n",
    "\n",
    "In a sense, the one-hot $y$ representation is a \"perfect\" probability distribution for the correct answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is the format for $Y$, which is the one hot encoding of the correct class $y*$ for each example $x$ in the batch?**\n",
    "\n",
    "Shape is $(bs, 10)$ and each row $i$ is a one hot encoding of $y_i^*$ ($i$-th correct class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What properties do we want out of our last hidden layer $h^{(3)}$ (aka, the output layer)?**\n",
    "\n",
    "All values $h^{(3)}_i$ must be between zero and one because otherwise they're not a valid probability.\n",
    "\n",
    "The probabilities should sum to one so that $h^{(3)}$ forms a proper probability *distribution.*\n",
    "\n",
    "For an input $x$, we ideally want $h^{(3)}$ to be equal to the one-hot encoding $y$: be all zeros except for at the position of the correct answer, where it has a value 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The probability we assign to the correct class is $h^{(3)}_{y^*}$. What is the ideal value of $h^{(3)}_{y^*}$?**\n",
    "\n",
    "1.0 or 100%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is the ideal value of $\\log h^{(3)}_{y^*}$?**\n",
    "\n",
    "0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the worst value of $h^{(3)}_{y^*}$? And $\\log h^{(3)}_{y^*}$?**\n",
    "\n",
    "0.0 and $-\\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. If larger values of $h^{(3)}_{y^*}$ are better than what values of $\\log h^{(3)}_{y^*}$ are better? Why?**\n",
    "\n",
    "Larger ones. Because monotonic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What are the properties of a loss function?**\n",
    "\n",
    "Loss function should be non-negative. Should be zero when perfect/correct.\n",
    "\n",
    "The worse the prediction the greater the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Can we use $\\log h^{(3)}_{y^*}$ by itself as a loss function? Why? What do we have to do to use $\\log h^{(3)}_{y^*}$ as a loss function?**\n",
    "\n",
    "No. Goes negative. Greater values are better.\n",
    "\n",
    "We use $-\\log h^{(3)}_{y^*}$ as the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Is there a deep reason for using $-\\log h^{(3)}_{y^*}$ rather than say $-h^{(3)}_{y^*}$?**\n",
    "\n",
    "**TODO**: Maximum likelihood of dataset, add up cross entropy losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What do we call this loss function?**\n",
    "\n",
    "Cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. If I give you $h^{(3)}$ and a one-hot encoding $y$ for a single example, what is the formula for the cross-entropy loss?**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "{CE}_{\\text{vector}} (h^{(3)}, y) &= -\\log\\left(\n",
    "    h^{(3)} \\cdot y\n",
    "\\right)\n",
    "\\\\\n",
    "&= -\\log\\left(\n",
    "    \\sum_{i = 0}^{9} h^{(3)}_i y_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Now that you can write ${CE}_{\\text{vector}}$, can you write ${CE}_{\\text{matrix}}$? You may use a summation $\\sum$ over each row of $H^{(3)}$ and $Y$, and you may use ${CE}_{\\text{vector}}$.**\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "{CE}_{\\text{matrix}}(H^{(3)}, Y)\n",
    "=\n",
    "\\sum_{i = 0}^{bs} {CE}_{\\text{vector}}\\left(\n",
    "    H^{(3)}_{i, :},\n",
    "    Y_{i, :}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a. We use Python loops to implement $\\sum$s. Python loops are slow. Numpy operations are fast. Let's step-by-step learn to eliminate the explicit summation for ${CE}_{\\text{matrix}}$. Okay?**\n",
    "\n",
    "Yeah!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3b. ${CE}_{\\text{matrix}}$ calls ${CE}_{\\text{vector}}$ for each pair of corresponding rows of $H^{(3)}$ and $Y$. That involves take the negative log of a dot product.  Let's perform this dot product using numpy. A dot product first multiplies corresponding entries in a vector. How do we get numpy to do this for a matrix?**\n",
    "\n",
    "\\\\[\n",
    "H^{(3)} * Y \\quad\\text{(numpy)}\n",
    "\\\\\n",
    "H^{(3)} \\odot Y \\quad\\text{(math)}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c. The next step of a dot product is to sum out the products. We need to do this per row. We can do this using `np.sum`. What named argument must we pass `np.sum`? What is the shape of the result? How would you describe the result in words?**\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}(H^{(3)} * Y, axis = 1) \\quad\\text{(numpy)}\n",
    "\\\\]\n",
    "\n",
    "This is a vector of shape $(bs,)$. The $i$-th entry is equal to the probability assigned to the correct class $y_i^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d. Use the above formula to calculate the cross entropies for each example in the batch ${CE}_{\\text{vector}} (H^{(3)}_{i, :}, Y_{i, :})$. What is the shape of this?**\n",
    "\n",
    "\\\\[\n",
    "-\\log \\text{np.sum}(H^{(3)} * Y, axis = 1) \\quad\\text{(numpy)}\n",
    "\\\\]\n",
    "\n",
    "The shape is $(bs,)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e. Last, use `np.sum` to calculate the total mean cross entropy for the batch.**\n",
    "\n",
    "\\\\[\n",
    "\\text{np.sum}\\left(\n",
    "    -\\log \\text{np.sum}(H^{(3)} * Y, axis = 1),\n",
    "    axis = 0\n",
    "\\right)\n",
    "/ bs\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
